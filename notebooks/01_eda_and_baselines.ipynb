{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a243cf22",
   "metadata": {},
   "source": [
    "# TurboFault — EDA & Baselines\n",
    "\n",
    "Exploratory data analysis of the **NASA C-MAPSS Turbofan Engine Degradation** dataset and tabular baseline benchmarks.\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Setup & load C-MAPSS FD001 |\n",
    "| 2 | Data overview & shape inspection |\n",
    "| 3 | Missing values & data types |\n",
    "| 4 | Descriptive statistics (sensor channels) |\n",
    "| 5 | Sensor degradation traces |\n",
    "| 6 | RUL distribution & piecewise-linear capping |\n",
    "| 7 | Operational settings analysis |\n",
    "| 8 | Correlation matrix & bivariate analysis |\n",
    "| 9 | Feature engineering pipeline |\n",
    "| 10 | Outlier detection (IQR / z-score) |\n",
    "| 11 | Baseline models (XGBoost, Random Forest, Ridge) |\n",
    "| 12 | Key findings & summary |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e707d47",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e243818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Ensure turbofault is importable from notebooks/\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from turbofault.data.dataset import (\n",
    "    load_cmapss,\n",
    "    download_cmapss,\n",
    "    CMAPSSDataset,\n",
    "    SENSOR_COLUMNS,\n",
    "    OPERATIONAL_SETTINGS,\n",
    "    ALL_COLUMNS,\n",
    "    LOW_VARIANCE_SENSORS,\n",
    "    SUBSET_INFO,\n",
    ")\n",
    "from turbofault.data.features import (\n",
    "    build_feature_set,\n",
    "    get_feature_columns,\n",
    "    add_rolling_features,\n",
    "    add_lag_features,\n",
    "    add_delta_features,\n",
    ")\n",
    "from turbofault.data.preprocessing import (\n",
    "    normalize_sensors,\n",
    "    drop_low_variance_sensors,\n",
    "    temporal_train_val_split,\n",
    "    get_last_cycle_per_engine,\n",
    "    prepare_tabular_data,\n",
    ")\n",
    "from turbofault.models.xgboost_baseline import XGBoostRUL, RandomForestRUL, RidgeRUL\n",
    "from turbofault.training.evaluation import evaluate_rul, nasa_score, print_comparison_table\n",
    "from turbofault.visualization.plots import (\n",
    "    plot_sensor_traces,\n",
    "    plot_rul_predictions,\n",
    "    plot_feature_importance,\n",
    "    plot_model_comparison,\n",
    ")\n",
    "\n",
    "# ── Style ────────────────────────────────────────────────────────────\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "pd.set_option(\"display.max_columns\", 40)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653262ee",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "The C-MAPSS (Commercial Modular Aero-Propulsion System Simulation) dataset contains run-to-failure simulations for turbofan engines. We'll start with **FD001** — the simplest subset (1 operating condition, 1 fault mode).\n",
    "\n",
    "If the data hasn't been downloaded yet, the cell below will attempt to fetch it from NASA's Prognostics Data Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/CMAPSSData\")\n",
    "SUBSET = \"FD001\"\n",
    "MAX_RUL = 125  # Piecewise-linear RUL cap\n",
    "\n",
    "# Download if not present\n",
    "if not DATA_DIR.exists():\n",
    "    print(\"Downloading C-MAPSS dataset...\")\n",
    "    download_cmapss(DATA_DIR.parent)\n",
    "\n",
    "# Load FD001 with RUL computation\n",
    "dataset = load_cmapss(DATA_DIR, subset=SUBSET, max_rul=MAX_RUL)\n",
    "print(dataset.summary())\n",
    "print(f\"\\nSubset info: {SUBSET_INFO[SUBSET]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97931b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset.train_df.copy()\n",
    "test_df = dataset.test_df.copy()\n",
    "\n",
    "print(\"Training data — first 5 rows:\")\n",
    "display(train_df.head())\n",
    "print(f\"\\nTraining data — last 5 rows:\")\n",
    "display(train_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73759c74",
   "metadata": {},
   "source": [
    "## 3. Data Overview & Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fa422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'─' * 50}\")\n",
    "print(f\"Training set shape:  {train_df.shape}\")\n",
    "print(f\"Test set shape:      {test_df.shape}\")\n",
    "print(f\"RUL ground truth:    {dataset.rul_df.shape}\")\n",
    "print(f\"{'─' * 50}\")\n",
    "print(f\"Train engines:       {dataset.num_train_engines}\")\n",
    "print(f\"Test engines:        {dataset.num_test_engines}\")\n",
    "print(f\"Sensors:             {dataset.num_sensors}\")\n",
    "print(f\"Op settings:         {len(OPERATIONAL_SETTINGS)}\")\n",
    "print(f\"{'─' * 50}\")\n",
    "\n",
    "# Engine lifecycle lengths\n",
    "engine_lengths = train_df.groupby(\"engine_id\")[\"cycle\"].max()\n",
    "print(f\"\\nEngine lifecycle (cycles):\")\n",
    "print(f\"  Min:    {engine_lengths.min()}\")\n",
    "print(f\"  Max:    {engine_lengths.max()}\")\n",
    "print(f\"  Mean:   {engine_lengths.mean():.1f}\")\n",
    "print(f\"  Median: {engine_lengths.median():.1f}\")\n",
    "print(f\"  Std:    {engine_lengths.std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame info (training set):\")\n",
    "train_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f78a76",
   "metadata": {},
   "source": [
    "## 4. Data Types & Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b32609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "dtype_summary = pd.DataFrame({\n",
    "    \"dtype\": train_df.dtypes,\n",
    "    \"non_null\": train_df.count(),\n",
    "    \"null_count\": train_df.isnull().sum(),\n",
    "    \"null_pct\": (train_df.isnull().sum() / len(train_df) * 100).round(2),\n",
    "    \"unique\": train_df.nunique(),\n",
    "}).sort_values(\"null_pct\", ascending=False)\n",
    "\n",
    "display(dtype_summary)\n",
    "\n",
    "# Missing values heatmap\n",
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "sns.heatmap(\n",
    "    train_df.isnull().T,\n",
    "    cbar_kws={\"label\": \"Missing\"},\n",
    "    yticklabels=True,\n",
    "    cmap=\"YlOrRd\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Missing Value Heatmap (Training Set)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Sample Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total_missing = train_df.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values: {total_missing:,}\")\n",
    "if total_missing == 0:\n",
    "    print(\"✓ No missing values — C-MAPSS is a clean simulation dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2135898",
   "metadata": {},
   "source": [
    "## 5. Descriptive Statistics\n",
    "\n",
    "Sensor readings span different physical quantities (temperature, pressure, speed, ratios). Some sensors show near-zero variance in FD001 and carry no discriminative signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full descriptive statistics for sensor channels\n",
    "sensor_stats = train_df[SENSOR_COLUMNS].describe().T\n",
    "sensor_stats[\"range\"] = sensor_stats[\"max\"] - sensor_stats[\"min\"]\n",
    "sensor_stats[\"cv\"] = sensor_stats[\"std\"] / sensor_stats[\"mean\"].abs()  # Coefficient of variation\n",
    "sensor_stats[\"skew\"] = train_df[SENSOR_COLUMNS].skew()\n",
    "sensor_stats[\"kurtosis\"] = train_df[SENSOR_COLUMNS].kurtosis()\n",
    "\n",
    "display(sensor_stats.round(4))\n",
    "\n",
    "# Highlight low-variance sensors\n",
    "print(f\"\\n{'─' * 50}\")\n",
    "print(f\"Low-variance sensors (near-constant in FD001):\")\n",
    "for s in LOW_VARIANCE_SENSORS:\n",
    "    rng = sensor_stats.loc[s, \"range\"]\n",
    "    std = sensor_stats.loc[s, \"std\"]\n",
    "    print(f\"  {s:12s}  range={rng:.6f}  std={std:.6f}\")\n",
    "print(f\"\\nThese {len(LOW_VARIANCE_SENSORS)} sensors are typically dropped before modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba8f77",
   "metadata": {},
   "source": [
    "## 6. Sensor Degradation Traces\n",
    "\n",
    "Visualize how individual sensor readings evolve across engine lifecycles. Sensors that show a clear trend as RUL approaches 0 are the most predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51583ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few representative engines and informative sensors\n",
    "sample_engines = [1, 10, 50, 80]\n",
    "informative_sensors = [\"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_7\",\n",
    "                       \"sensor_11\", \"sensor_12\", \"sensor_15\", \"sensor_21\"]\n",
    "\n",
    "fig = plot_sensor_traces(train_df, engine_ids=sample_engines, sensors=informative_sensors,\n",
    "                         figsize=(16, 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22289aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor distributions — histograms for all 21 sensors\n",
    "active_sensors = [s for s in SENSOR_COLUMNS if s not in LOW_VARIANCE_SENSORS]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(18, 14))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, sensor in enumerate(active_sensors[:16]):\n",
    "    ax = axes_flat[idx]\n",
    "    ax.hist(train_df[sensor], bins=50, alpha=0.7, edgecolor=\"black\", linewidth=0.5)\n",
    "    skew_val = train_df[sensor].skew()\n",
    "    ax.set_title(f\"{sensor} (skew={skew_val:.2f})\", fontsize=10)\n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(active_sensors[:16]), len(axes_flat)):\n",
    "    axes_flat[idx].set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Sensor Distributions (Active Sensors Only)\", fontsize=14, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bfc55",
   "metadata": {},
   "source": [
    "## 7. RUL Distribution & Piecewise-Linear Capping\n",
    "\n",
    "The raw RUL is a linear countdown from max lifecycle to 0. Piecewise-linear capping at `max_rul=125` prevents the model from trying to predict exact RUL at engine start (where degradation hasn't begun and the signal is noisy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. RUL distribution (histogram)\n",
    "axes[0].hist(train_df[\"rul\"], bins=60, alpha=0.7, edgecolor=\"black\", linewidth=0.5,\n",
    "             color=\"#3498db\")\n",
    "axes[0].axvline(MAX_RUL, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Cap = {MAX_RUL}\")\n",
    "axes[0].set_xlabel(\"RUL (cycles)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"RUL Distribution (Capped)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Engine lifecycle length distribution\n",
    "axes[1].hist(engine_lengths, bins=30, alpha=0.7, edgecolor=\"black\", linewidth=0.5,\n",
    "             color=\"#2ecc71\")\n",
    "axes[1].set_xlabel(\"Total Cycles\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Engine Lifecycle Length\")\n",
    "\n",
    "# 3. RUL vs Cycle for a few engines (to show piecewise-linear shape)\n",
    "for eid in [1, 25, 50, 75]:\n",
    "    eng = train_df[train_df[\"engine_id\"] == eid].sort_values(\"cycle\")\n",
    "    axes[2].plot(eng[\"cycle\"], eng[\"rul\"], label=f\"Engine {eid}\", alpha=0.7)\n",
    "axes[2].axhline(MAX_RUL, color=\"red\", linestyle=\"--\", linewidth=1.5, alpha=0.7)\n",
    "axes[2].set_xlabel(\"Cycle\")\n",
    "axes[2].set_ylabel(\"RUL\")\n",
    "axes[2].set_title(\"Piecewise-Linear RUL Curves\")\n",
    "axes[2].legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(f\"Remaining Useful Life Analysis — {SUBSET}\", fontsize=14, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"RUL capped at: {MAX_RUL} cycles\")\n",
    "print(f\"Fraction of samples at cap: {(train_df['rul'] == MAX_RUL).mean():.1%}\")\n",
    "print(f\"RUL range: [{train_df['rul'].min()}, {train_df['rul'].max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f55493",
   "metadata": {},
   "source": [
    "## 8. Operational Settings Analysis\n",
    "\n",
    "FD001 has a single operating condition (sea level), so operational settings should be nearly constant. In FD002/FD004 (6 conditions), these become crucial for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32226977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operational settings summary\n",
    "op_stats = train_df[OPERATIONAL_SETTINGS].describe().T\n",
    "display(op_stats)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "for idx, col in enumerate(OPERATIONAL_SETTINGS):\n",
    "    axes[idx].hist(train_df[col], bins=50, alpha=0.7, edgecolor=\"black\", linewidth=0.5)\n",
    "    n_unique = train_df[col].nunique()\n",
    "    axes[idx].set_title(f\"{col} ({n_unique} unique values)\")\n",
    "    axes[idx].set_xlabel(\"Value\")\n",
    "\n",
    "fig.suptitle(f\"Operational Settings — {SUBSET}\", fontsize=14, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c53e9",
   "metadata": {},
   "source": [
    "## 9. Correlation Matrix & Bivariate Analysis\n",
    "\n",
    "Identify which sensors are most correlated with RUL (potential top predictors) and which sensors are highly inter-correlated (redundancy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc35e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for active sensors + RUL\n",
    "corr_cols = active_sensors + [\"rul\"]\n",
    "corr_matrix = train_df[corr_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    ax=ax,\n",
    "    annot_kws={\"size\": 7},\n",
    ")\n",
    "ax.set_title(f\"Sensor Correlation Matrix — {SUBSET}\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with RUL\n",
    "rul_corr = corr_matrix[\"rul\"].drop(\"rul\").abs().sort_values(ascending=False)\n",
    "print(\"Sensor correlations with RUL (abs):\")\n",
    "print(\"─\" * 40)\n",
    "for sensor, corr in rul_corr.items():\n",
    "    direction = \"+\" if corr_matrix.loc[sensor, \"rul\"] > 0 else \"−\"\n",
    "    bar = \"█\" * int(corr * 30)\n",
    "    print(f\"  {sensor:12s}  {direction}{corr:.3f}  {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots: top 4 sensors vs RUL\n",
    "top_sensors = rul_corr.head(4).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for idx, sensor in enumerate(top_sensors):\n",
    "    ax = axes[idx]\n",
    "    # Subsample for speed\n",
    "    sample_idx = np.random.choice(len(train_df), size=min(3000, len(train_df)), replace=False)\n",
    "    sample = train_df.iloc[sample_idx]\n",
    "    ax.scatter(sample[sensor], sample[\"rul\"], alpha=0.15, s=5, c=\"#3498db\")\n",
    "    ax.set_xlabel(sensor)\n",
    "    ax.set_ylabel(\"RUL\")\n",
    "    r = corr_matrix.loc[sensor, \"rul\"]\n",
    "    ax.set_title(f\"{sensor} vs RUL (r={r:.3f})\")\n",
    "\n",
    "fig.suptitle(\"Top Correlated Sensors vs. RUL\", fontsize=14, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609f2b8",
   "metadata": {},
   "source": [
    "## 10. Outlier Detection\n",
    "\n",
    "Use IQR and z-score methods to identify sensor outliers. In C-MAPSS, \"outliers\" near failure are actually the signal we want to predict — extreme sensor values at low RUL are expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f246d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR-based outlier detection for active sensors\n",
    "outlier_summary = []\n",
    "for sensor in active_sensors:\n",
    "    Q1 = train_df[sensor].quantile(0.25)\n",
    "    Q3 = train_df[sensor].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    n_outliers = ((train_df[sensor] < lower) | (train_df[sensor] > upper)).sum()\n",
    "    z_outliers = (np.abs(stats.zscore(train_df[sensor])) > 3).sum()\n",
    "    outlier_summary.append({\n",
    "        \"sensor\": sensor,\n",
    "        \"iqr_outliers\": n_outliers,\n",
    "        \"iqr_outlier_pct\": n_outliers / len(train_df) * 100,\n",
    "        \"zscore_outliers\": z_outliers,\n",
    "        \"zscore_outlier_pct\": z_outliers / len(train_df) * 100,\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).set_index(\"sensor\")\n",
    "display(outlier_df.round(2).sort_values(\"iqr_outlier_pct\", ascending=False))\n",
    "\n",
    "# Box plots for top outlier sensors\n",
    "top_outlier_sensors = outlier_df.nlargest(6, \"iqr_outlier_pct\").index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "for idx, sensor in enumerate(top_outlier_sensors):\n",
    "    ax = axes.flatten()[idx]\n",
    "    sns.boxplot(data=train_df, x=sensor, ax=ax, color=\"#3498db\")\n",
    "    ax.set_title(f\"{sensor} ({outlier_df.loc[sensor, 'iqr_outlier_pct']:.1f}% outliers)\")\n",
    "\n",
    "fig.suptitle(\"Sensors with Most IQR Outliers\", fontsize=14, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n⚠ Note: In prognostics data, 'outliers' at end-of-life are the degradation\")\n",
    "print(f\"  signal. Do NOT remove them — they carry the most predictive information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c7bbc",
   "metadata": {},
   "source": [
    "## 11. Feature Engineering Pipeline\n",
    "\n",
    "Build the full engineered feature set using TurboFault's `build_feature_set()` function: rolling statistics, lag features, rate-of-change deltas, EWMA smoothing, and cycle normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbee9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop low-variance sensors first\n",
    "train_clean = drop_low_variance_sensors(train_df)\n",
    "\n",
    "# Use active sensors for feature engineering\n",
    "active = [s for s in SENSOR_COLUMNS if s not in LOW_VARIANCE_SENSORS]\n",
    "\n",
    "# Build full feature set\n",
    "train_feat = build_feature_set(\n",
    "    train_clean,\n",
    "    sensors=active,\n",
    "    rolling_windows=(5, 10, 20),\n",
    "    rolling_stats=(\"mean\", \"std\"),\n",
    "    lags=(1, 3, 5),\n",
    "    delta_periods=(1, 5),\n",
    "    ewma_spans=(10, 20),\n",
    "    include_cycle=True,\n",
    ")\n",
    "\n",
    "feature_cols = get_feature_columns(train_feat, include_operational=True, include_raw_sensors=True)\n",
    "\n",
    "print(f\"Original features:     {len(SENSOR_COLUMNS) + len(OPERATIONAL_SETTINGS)}\")\n",
    "print(f\"Engineered features:   {len(feature_cols)}\")\n",
    "print(f\"Training samples:      {len(train_feat):,}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  Raw sensors:    {len(active)}\")\n",
    "print(f\"  Rolling stats:  {sum(1 for c in feature_cols if '_roll' in c)}\")\n",
    "print(f\"  Lag features:   {sum(1 for c in feature_cols if '_lag' in c)}\")\n",
    "print(f\"  Delta features: {sum(1 for c in feature_cols if '_delta' in c)}\")\n",
    "print(f\"  EWMA features:  {sum(1 for c in feature_cols if '_ewma' in c)}\")\n",
    "print(f\"  Cycle features: {sum(1 for c in feature_cols if 'cycle' in c)}\")\n",
    "print(f\"  Op settings:    {sum(1 for c in feature_cols if 'op_setting' in c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e61b2c",
   "metadata": {},
   "source": [
    "## 12. Baseline Models\n",
    "\n",
    "Train XGBoost, Random Forest, and Ridge Regression on the engineered feature set. These tabular baselines set the performance bar for deep learning models (LSTM, Transformer, 1D-CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data with temporal split and normalization\n",
    "# Re-engineer features on both train and test sets using the dataset object\n",
    "dataset.train_df = drop_low_variance_sensors(dataset.train_df)\n",
    "dataset.test_df = drop_low_variance_sensors(dataset.test_df)\n",
    "\n",
    "# Full feature engineering on train\n",
    "train_eng = build_feature_set(dataset.train_df, sensors=active)\n",
    "feat_cols = get_feature_columns(train_eng)\n",
    "\n",
    "# Normalize\n",
    "train_norm, _, scaler = normalize_sensors(train_eng, columns=feat_cols)\n",
    "\n",
    "# Temporal train/val split\n",
    "train_split, val_split = temporal_train_val_split(train_norm, val_fraction=0.2)\n",
    "\n",
    "# Test: feature engineer and get last cycle\n",
    "test_eng = build_feature_set(dataset.test_df, sensors=active)\n",
    "_, test_norm, _ = normalize_sensors(train_eng, test_eng, columns=feat_cols)\n",
    "test_last = get_last_cycle_per_engine(test_norm)\n",
    "\n",
    "# Ensure consistent feature columns\n",
    "feat_cols = [c for c in feat_cols if c in train_split.columns and c in test_last.columns]\n",
    "\n",
    "X_train = train_split[feat_cols].values\n",
    "y_train = train_split[\"rul\"].values\n",
    "X_val = val_split[feat_cols].values\n",
    "y_val = val_split[\"rul\"].values\n",
    "X_test = test_last[feat_cols].values\n",
    "y_test = test_last[\"rul\"].values\n",
    "\n",
    "print(f\"X_train: {X_train.shape}  |  X_val: {X_val.shape}  |  X_test: {X_test.shape}\")\n",
    "print(f\"Features: {len(feat_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── XGBoost ──────────────────────────────────────────────────────────\n",
    "xgb_model = XGBoostRUL(n_estimators=500, max_depth=6, learning_rate=0.05)\n",
    "xgb_model.fit(X_train, y_train, X_val, y_val)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Results:\")\n",
    "xgb_results = evaluate_rul(y_test, xgb_pred, prefix=\"xgb_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367438dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Random Forest ────────────────────────────────────────────────────\n",
    "rf_model = RandomForestRUL(n_estimators=300, max_depth=12)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "rf_results = evaluate_rul(y_test, rf_pred, prefix=\"rf_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Ridge Regression ─────────────────────────────────────────────────\n",
    "ridge_model = RidgeRUL(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_pred = ridge_model.predict(X_test)\n",
    "\n",
    "print(\"Ridge Regression Results:\")\n",
    "ridge_results = evaluate_rul(y_test, ridge_pred, prefix=\"ridge_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model Comparison ─────────────────────────────────────────────────\n",
    "all_results = {\n",
    "    \"XGBoost\": {k.replace(\"xgb_\", \"\"): v for k, v in xgb_results.items()},\n",
    "    \"Random Forest\": {k.replace(\"rf_\", \"\"): v for k, v in rf_results.items()},\n",
    "    \"Ridge\": {k.replace(\"ridge_\", \"\"): v for k, v in ridge_results.items()},\n",
    "}\n",
    "\n",
    "comparison = print_comparison_table(all_results)\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction scatter plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "models = [(\"XGBoost\", xgb_pred), (\"Random Forest\", rf_pred), (\"Ridge\", ridge_pred)]\n",
    "\n",
    "for idx, (name, pred) in enumerate(models):\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(y_test, pred, alpha=0.6, s=30, edgecolors=\"black\", linewidth=0.3)\n",
    "    lims = [0, max(y_test.max(), pred.max()) + 10]\n",
    "    ax.plot(lims, lims, \"r--\", linewidth=1.5, label=\"Perfect\")\n",
    "    ax.set_xlabel(\"True RUL\")\n",
    "    ax.set_ylabel(\"Predicted RUL\")\n",
    "    rmse = all_results[name][\"rmse\"]\n",
    "    ax.set_title(f\"{name} (RMSE={rmse:.1f})\")\n",
    "    ax.legend()\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "\n",
    "fig.suptitle(f\"Baseline RUL Predictions — {SUBSET}\", fontsize=14, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance — top 20\n",
    "importance = xgb_model.get_feature_importance(feature_names=feat_cols, top_n=20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "names = [p[0] for p in reversed(importance)]\n",
    "scores = [p[1] for p in reversed(importance)]\n",
    "colors = [\"#e74c3c\" if \"roll\" in n else \"#3498db\" if \"lag\" in n\n",
    "          else \"#2ecc71\" if \"delta\" in n else \"#f39c12\" if \"ewma\" in n\n",
    "          else \"#9b59b6\" for n in names]\n",
    "ax.barh(names, scores, color=colors)\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_title(\"XGBoost — Top 20 Feature Importances\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#e74c3c\", label=\"Rolling\"),\n",
    "    Patch(facecolor=\"#3498db\", label=\"Lag\"),\n",
    "    Patch(facecolor=\"#2ecc71\", label=\"Delta\"),\n",
    "    Patch(facecolor=\"#f39c12\", label=\"EWMA\"),\n",
    "    Patch(facecolor=\"#9b59b6\", label=\"Raw/Other\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"lower right\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae5d51",
   "metadata": {},
   "source": [
    "## 13. Key Findings & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90947d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile per-column summary table\n",
    "summary_data = []\n",
    "all_cols = active_sensors + OPERATIONAL_SETTINGS\n",
    "for col in all_cols:\n",
    "    col_data = train_df[col]\n",
    "    rul_r = train_df[[\"rul\", col]].corr().iloc[0, 1] if \"rul\" in train_df.columns else np.nan\n",
    "    iqr_out = outlier_df.loc[col, \"iqr_outliers\"] if col in outlier_df.index else 0\n",
    "    summary_data.append({\n",
    "        \"column\": col,\n",
    "        \"dtype\": str(col_data.dtype),\n",
    "        \"missing\": col_data.isnull().sum(),\n",
    "        \"unique\": col_data.nunique(),\n",
    "        \"mean\": col_data.mean(),\n",
    "        \"std\": col_data.std(),\n",
    "        \"rul_corr\": rul_r,\n",
    "        \"iqr_outliers\": iqr_out,\n",
    "        \"low_variance\": col in LOW_VARIANCE_SENSORS,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).set_index(\"column\")\n",
    "summary_df = summary_df.sort_values(\"rul_corr\", key=abs, ascending=False)\n",
    "display(summary_df.round(4))\n",
    "\n",
    "print(f\"\\n{'═' * 60}\")\n",
    "print(f\"KEY FINDINGS — C-MAPSS {SUBSET}\")\n",
    "print(f\"{'═' * 60}\")\n",
    "print(f\"  • {len(train_df):,} training samples across {dataset.num_train_engines} engines\")\n",
    "print(f\"  • {len(active_sensors)} informative sensors ({len(LOW_VARIANCE_SENSORS)} low-variance dropped)\")\n",
    "print(f\"  • No missing values (simulation data)\")\n",
    "print(f\"  • Engine lifecycles range from {engine_lengths.min()} to {engine_lengths.max()} cycles\")\n",
    "print(f\"  • RUL capped at {MAX_RUL} cycles (piecewise-linear)\")\n",
    "print(f\"  • {len(feat_cols)} total engineered features\")\n",
    "print(f\"\")\n",
    "print(f\"BASELINE PERFORMANCE (Test Set):\")\n",
    "for name, res in all_results.items():\n",
    "    print(f\"  {name:15s}  RMSE={res['rmse']:.2f}  MAE={res['mae']:.2f}  \"\n",
    "          f\"R²={res['r2']:.4f}  NASA={res['nasa_score']:.0f}\")\n",
    "print(f\"{'═' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90c711",
   "metadata": {},
   "source": [
    "## 14. Feature Explainability\n",
    "\n",
    "Understand _which sensors and engineered features_ drive the XGBoost RUL predictions using **permutation importance** and **sensor contribution analysis**.\n",
    "\n",
    "Permutation importance shuffles each feature independently and measures the increase in prediction error — features that cause the largest error increase when shuffled are the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from turbofault.explain.feature_explainer import (\n",
    "    permutation_importance,\n",
    "    sensor_contribution_analysis,\n",
    "    generate_explanation_report,\n",
    ")\n",
    "\n",
    "# ── Permutation Importance ──────────────────────────────────────────\n",
    "perm_imp = permutation_importance(\n",
    "    xgb_model, X_test, y_test,\n",
    "    feature_names=feat_cols,\n",
    "    n_repeats=10,\n",
    "    metric=\"rmse\",\n",
    ")\n",
    "\n",
    "print(\"Top 20 Features by Permutation Importance (RMSE increase when shuffled):\")\n",
    "print(\"─\" * 60)\n",
    "display(perm_imp.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Permutation Importance Bar Chart ─────────────────────────────────\n",
    "top_n = 20\n",
    "top_perm = perm_imp.head(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = []\n",
    "for name in reversed(top_perm[\"feature\"].values):\n",
    "    if \"_roll_\" in name:\n",
    "        colors.append(\"#e74c3c\")\n",
    "    elif \"_lag_\" in name:\n",
    "        colors.append(\"#3498db\")\n",
    "    elif \"_delta_\" in name:\n",
    "        colors.append(\"#2ecc71\")\n",
    "    elif \"_ewma_\" in name:\n",
    "        colors.append(\"#f39c12\")\n",
    "    else:\n",
    "        colors.append(\"#9b59b6\")\n",
    "\n",
    "ax.barh(\n",
    "    range(top_n),\n",
    "    list(reversed(top_perm[\"importance_mean\"].values)),\n",
    "    xerr=list(reversed(top_perm[\"importance_std\"].values)),\n",
    "    color=colors,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.3,\n",
    "    capsize=3,\n",
    ")\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(list(reversed(top_perm[\"feature\"].values)), fontsize=9)\n",
    "ax.set_xlabel(\"Importance (RMSE increase when shuffled)\")\n",
    "ax.set_title(f\"Permutation Feature Importance — XGBoost ({SUBSET})\",\n",
    "             fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#e74c3c\", label=\"Rolling\"),\n",
    "    Patch(facecolor=\"#3498db\", label=\"Lag\"),\n",
    "    Patch(facecolor=\"#2ecc71\", label=\"Delta\"),\n",
    "    Patch(facecolor=\"#f39c12\", label=\"EWMA\"),\n",
    "    Patch(facecolor=\"#9b59b6\", label=\"Raw/Other\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"lower right\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf37ef",
   "metadata": {},
   "source": [
    "### Sensor Contribution Analysis\n",
    "\n",
    "Group feature importances by their **base sensor** to answer: \"Which physical sensor channels contribute most to RUL prediction?\" This aggregates importance across all derived features (rolling, lag, delta, EWMA) for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba730ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sensor Contribution Analysis ────────────────────────────────────\n",
    "analysis = sensor_contribution_analysis(perm_imp, top_n=20)\n",
    "\n",
    "print(\"Sensor Ranking (by total importance across all derived features):\")\n",
    "print(\"─\" * 60)\n",
    "display(analysis[\"sensor_ranking\"].round(4))\n",
    "\n",
    "print(\"\\nFeature Type Ranking:\")\n",
    "print(\"─\" * 60)\n",
    "display(analysis[\"feature_type_ranking\"].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6be868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sensor Importance Visualization ─────────────────────────────────\n",
    "sensor_rank = analysis[\"sensor_ranking\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Sensor ranking bar chart\n",
    "sensors = sensor_rank.index.tolist()\n",
    "totals = sensor_rank[\"total_importance\"].values\n",
    "ax1.barh(range(len(sensors)), totals[::-1], color=\"#3498db\",\n",
    "         edgecolor=\"black\", linewidth=0.3)\n",
    "ax1.set_yticks(range(len(sensors)))\n",
    "ax1.set_yticklabels(sensors[::-1], fontsize=9)\n",
    "ax1.set_xlabel(\"Total Importance (summed across derived features)\")\n",
    "ax1.set_title(\"Sensor Ranking\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Right: Feature type pie chart\n",
    "type_rank = analysis[\"feature_type_ranking\"]\n",
    "type_colors = {\n",
    "    \"rolling\": \"#e74c3c\", \"lag\": \"#3498db\", \"delta\": \"#2ecc71\",\n",
    "    \"ewma\": \"#f39c12\", \"raw\": \"#9b59b6\", \"operational\": \"#1abc9c\",\n",
    "    \"cycle\": \"#95a5a6\",\n",
    "}\n",
    "colors = [type_colors.get(t, \"#bdc3c7\") for t in type_rank.index]\n",
    "ax2.pie(type_rank[\"total_importance\"], labels=type_rank.index,\n",
    "        colors=colors, autopct=\"%1.1f%%\", startangle=90, pctdistance=0.85)\n",
    "ax2.set_title(\"Importance by Feature Type\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "fig.suptitle(f\"Explainability Analysis — XGBoost ({SUBSET})\",\n",
    "             fontsize=14, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Full Explanation Report ──────────────────────────────────────────\n",
    "report = generate_explanation_report(\"XGBoost\", perm_imp, analysis, top_n=15)\n",
    "print(report)\n",
    "\n",
    "print(f\"\\nNEXT STEPS:\")\n",
    "print(f\"  1. Train LSTM, GRU, Transformer, 1D-CNN on sequence data\")\n",
    "print(f\"  2. Compare deep models against these baselines\")\n",
    "print(f\"  3. Run Optuna hyperparameter search (turbofault tune)\")\n",
    "print(f\"  4. Extract Transformer attention weights for temporal explainability\")\n",
    "print(f\"  5. Extend to FD002–FD004 (multi-condition, multi-fault)\")\n",
    "print(f\"{'═' * 60}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
